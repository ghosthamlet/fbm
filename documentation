
                    INTRODUCTORY DOCUMENTATION FOR

     SOFTWARE IMPLEMENTING BAYESIAN LEARNING FOR NEURAL NETWORKS
               USING MARKOV CHAIN MONTE CARLO METHODS

                Copyright (c) 1995 by Radford M. Neal 

Permission is granted for anyone to copy, use, or modify these
programs and accompanying documents for purposes of research or
education, provided this copyright notice is retained, and note is
made of any changes that have been made.

These programs and documents are distributed without any warranty,
express or implied.  As the programs were written for research
purposes only, they have not been tested to the degree that would be
advisable in any important application.  All use of these programs is
entirely at the user's own risk.

If you have comments, bug reports, or questions, or if you wish
permission to use these programs for commercial or any other purposes
except research or education, you may contact me at the following
address:
                   Radford Neal
                   Dept. of Statistics
                   University of Toronto
                   100 St. George Street
                   Toronto, Ontario  M5S 1A1
                   CANADA

or e-mail me at radford@stat.toronto.edu or radford@cs.toronto.edu.

Information on any updates or other related work may from time to time
be available via the World Wide Web, starting from my home page at URL
http://www.cs.toronto.edu/~radford.


INTRODUCTION

This software implements Bayesian methods for learning multilayer
perceptron networks, as described in my thesis, "Bayesian Learning for
Neural Networks".  The implementation uses Markov chain Monte Carlo
methods.  Software modules that support Markov chain sampling are
included in the distribution, and may be usefull in other
applications.  Note that I am distributing this software to facilitate
research in this area.  Potential users should make note of the
copyright notice above.  You must obtain permission from me before
using this software for purposes other than research or education.

The software supports Bayesian learning for regression and
classification problems using models based on networks with any number
of hidden layers, with a wide variety of prior distributions for
network parameters and hyperparameters.  The advantages of Bayesian
learning include the automatic determination of "regularization"
hyperparameters, without the need for a validation set, the avoidance
of overfitting when using large networks, and the quantification of
uncertainty in predictions.  The software implements the Automatic
Relevance Determination (ARD) approach to handling inputs that may
turn out to be irrelevant (developed with David MacKay).  For problems
and networks of moderate size (eg, 200 training cases, 10 inputs, 20
hidden units), full training (to the point where one can be reasonably
sure that the correct Bayesian answer has been found) typically takes
several hours to a day on our SGI machine.  However, quite good
results, competitive with other methods, are often obtained after
training for under an hour.  (Of course, your machine may not be as
fast as ours!)

To understand how to use this software, it is essential for you to
have read my thesis, which is available in PostScript via my home page
at URL http://www.cs.toronto.edu/~radford, or by anonymous ftp to
ftp.cs.toronto.edu, directory pub/radford, file thesis.ps.Z.  The
neural network models implemented are essentially as described in the
Appendix of the thesis.

The software consists of a number of programs and modules.  Three
major components are included in this distribution, each with its own
directory:
  
    util    Modules and programs of general utility.

    mc      Modules and programs that support sampling using Markov 
            chain Monte Carlo methods, using modules from util.

    net     Modules and programs that implement Bayesian inference
            for models based on multilayer perceptrons, using the
            modules from util and mc.

In addition, the 'bvg' directory contains modules and programs for
sampling from a bivariate Gaussian distribution, as a simple
demonstration of the capabilities of the Markov chain Monte Carlo
facilities.  The 'examples' directory contains example data sets that
are discussed below.

It is possible to use this software to do learning and prediction
without any knowledge of how the programs are written (assuming that
the software can be installed as described below without any
problems).  However, the complete source code is included so that
researchers can modify the programs to try out their own ideas.

The software is written in ANSI C, and is meant to be run in a UNIX
environment.  Specifically, it was developed on an SGI machine running
IRIX Release 5.3.  It also seems to run OK on a SPARC machine running
SunOS 5, using the 'gcc' C compiler.  As far as I know, the software
does not depend on any peculiarities of these environments (except
perhaps for the use of the drand48 psuedo-random number generator),
but you may nevertheless have problems getting it to work in
substantially different environments, and I can offer little or no
assistance in this regard.  There is no dependence on any particular
graphics package or graphical user interface.  (The 'xxx-plt' programs
are designed to allow their output to be piped directly into the
'xgraph' plotting program, but other plotting programs can be used
instead, or the numbers can be examined directly.)


INSTALLATION

The software is distributed as a Unix tar archive.  To obtain the
files, create an empty directory and change to it, download the tar
archive by anonymous ftp (or however you're able to get it) to file
"bnn.tar", and then issue the Unix command

    tar xf bnn.tar

If you obtained the archive in compressed form, as "bnn.tar.Z", you
must use the command "uncompress bnn.tar.Z" before doing the above.

The tar command should create sub-directories 'util', 'mc', 'net',
'bvg', and 'examples', and place a large number of files in these
sub-directories.  It should also place the files 'README',
'documentation', 'doc-index.html', and 'release-notes' in the current
directory.  If all this seems to have worked, you can remove the file
"bnn.tar".

The file 'release-notes' contains information on the current and past
releases.  It may be of interest if you are upgrading from an older
version of the software.  The file 'documentation' is a copy of this
document.

The 'util' directory contains a file of 100,000 natural random bytes,
which are used in combination with pseudo-random numbers.  To let the
random number generation routines find this file, you must edit the
file util/rand.c.  Find the following definition for RAND_FILE:
 
    #define RAND_FILE "???/util/randfile"

and change it by replacing ??? with the full path name of the
directory you created to hold this software.  The path name must start
with "/".   Abbreviated forms using "~" cannot be used.

Having done this, you can compile the programs by issuing the
following commands

    cd util
    make
    cd ..

    cd mc
    make
    cd ..

    cd net
    make
    cd ..

    cd bvg
    make
    cd ..

The last three commands can be omitted if you have no interest in the
demonstration of sampling from a bivariate Gaussian distribution. Note
that common modules will be compiled over again for each directory
where they are used; this is intentional.

It is possible that these compilation commands will fail for some
reason, in which case you'll have to figure out what's wrong and fix
it.  You might also want to edit the Makefiles in the sub-directories
in order to use your favourite C compiler, or set optimization flags
that are special to your compiler.

Note that for makes to work correctly, the programs MUST be kept in
separate 'util', 'mc', 'net', and 'bvg' sub-directories, with these
names.

Once the makes have been successful, you should now put the directories 
'util', 'mc', 'net', and perhaps 'bvg' in your search path.  How this
is done depends on the shell program you are using; consult a local
expert if you don't know how.  Subsequent instructions assume that
this has been done.


OVERVIEW OF THE SYSTEM

This software is being distributed primarily to further research in
Bayesian learning for neural network models.  The software is designed
for potentially wider use, however.  In particular, the programs and
modules in the 'util' directory are of general utility, and those in
the 'mc' directory provide generic support for Markov chain Monte
Carlo methods.  These facilities are specialized to neural network
learning by the modules and programs in the 'net' directory.  The
'bvg' directory demonstrates in a simple context how the generic
facilities in 'util' and 'mc' can be specialized for other tasks, but
users interested only in neural network learning need not concern
themselves with this.  

This section provides an overview of the facilities offered by the
various components of the software.  Some examples of how to train
networks are given in the next section, followed by a section giving
miscellaneous hints and warnings.  Detailed documentation is found in
separate ".doc" files, as described in the last section.


Log files

All the programs make use of a "log file" facility supported by
modules and programs in 'util'.  A log file records all the
information pertaining to a "run" of an iterative program.  The first
few records of the log file (with "indexes" of -1) contain the
specifications for the run (such as the network architecture and the
source of training data).  These records are written by "spec"
programs (eg, 'net-spec' and 'data-spec') that the user invokes at the
beginning of the run.  Once the run has been specified, the program
that performs iterations is invoked (eg, 'net-mc').  This program will
append further records to the log file, one for each iteration for
which the user has asked the state to be saved, which will usually be
every iteration, unless minimizing disk usage is a concern.  Each
record written has the iteration number as its index, and contains the
complete state of the program at that time (eg, all the parameters and
hyperparameters of the network being trained).  

Note that log files contain binary data; they are not human-readable.

After an iterative program finishes, the user may decide to let the
run continue for more iterations.  This is easily done by just
invoking the program again with a larger iteration limit, whereupon it
restarts using the last state stored in the log file, and then appends
records to the log file for further iterations.

The information about iterations that is stored in the log file can be
examined using various programs both during and after a run.  In
particular, the user can plot the progress of various quantities
during the course of the run, without having to decide beforehand
which quantities will be of interest.  The states saved at various
iterations are also the basis for making Monte Carlo estimates, and in
particular, for making Bayesian predictions based on a sample of
networks from the posterior distribution.


Data sources

The 'util' directory also contains modules and programs that support
reading of numeric input from data files or other sources, and for
specifying sets of training and test cases for supervised learning
procedures, such as models based on multilayer perceptron networks.

The data files used must contain numbers in standard ASCII form, with
one line per case, but there is considerable freedom regarding
separators and in the ordering of items.  "Input" and "target" items
that pertain to a case may come from the same file, or different
files, and the position within a line of each item may be specified
independently.  The set of cases (lines) to be used for training or
testing can be specified to be a subset of all the lines in a file.
The data source can also be specified to be the output of a program,
rather than a data file.

Specifications for where the training and test data comes from are
written to a log file by the 'data-spec' program, which also allows
the user to specify that certain transformations are to be done to the
data items before they are used.  In particular, the data can be
translated and re-scaled in a user-specified way, or by amounts that
are automatically determined from the training data.

The source of "test" data can also be specified explicitly by
arguments to the relevant commands, allowing the final results of
learning to be applied to any data set for which predictions are
desired.


Random number generation

A scheme for combining real and pseudo random numbers is implemented
by modules in the 'util' directory, along with procedures for sampling
from various standard distributions, and for saving the state of the
random number generator.  

The 'rand-seed' program is used to specify a random number seed to use
for a run.  The state of the random number generator is saved with
each iteration in the log file in order to ensure that resuming a run
produces the same results as if the run had continued without stopping.


Markov chain Monte Carlo

The 'mc' directory contains modules and programs that support the use
of Markov chain Monte Carlo methods.  A Markov chain Monte Carlo
application is created by adding modules to compute application-specific 
quantities, of which the most central is the probability distribution
to sample from.  For example, the neural network application provides
a procedure for computing the posterior probability density of the
network parameters.  An application may also provide implementations
of specialized sampling procedures, such as the procedures for doing
Gibbs sampling for hyperparameters in the neural network application.

A variety of Markov chain methods are supported by the 'mc' system,
including some that are not of much use in the neural network
application.  In particular, the "tempering" methods are not currently
implemented for the neural networks, though they may be in future.
Users interested only in neural networks should therefore ignore the
tempering facilities (such as the 'mc-temp-sched' and 'mc-temp-filter'
programs).

For the neural network user, the most important 'mc' program is
'mc-spec', which is used to specify how the Markov chain sampling is
to be done.  There are a large number of reasonable ways of sampling
for neural networks.  The best way is still the subject of research.
Good results can be obtained using several standard approaches,
however, as described in the examples in the next section.


Neural network models

The 'net' directory contains the modules and programs that implement
Bayesian learning for models based on multilayer perceptron networks,
making use of the modules in the 'util' and 'mc' directories.  The
networks and data models supported are as described in my thesis, with
the addition that the output units may now be connected to any of the
hidden layers (not just the last).  

A network training run is started with the 'net-spec' program, which
creates a log file to which it writes specifications for the network
architecture and priors.  In a simple run, the 'data-spec' and
'mc-spec' programs would then be used to specify the training set and
the way the sampling should be done, after which the 'net-mc' program
(a specialization of the generic 'xxx-mc' program) would be invoked to
do the actual sampling.  Finally, the 'net-pred' program would be used
to make predictions for test cases based on the networks saved in the
log file.

Usually, one would want to see how the run had gone before making
predictions.  The 'net-display' program allows one to examine the
network parameters and hyperparameters at any specified iteration.
The 'net-plt' program can be used to obtain the values of various
quantities, such as the training set error, for some range of
iterations.  The output of 'net-plt' would usually be piped to a
suitable plot program for visual examination, though it is also
possible to directly look at the numbers.

Several other programs are also present in the 'net' directory.  Some
of these will probably not be of interest to the ordinary user, as
they were written for debugging purposes, or to do specialized tasks
relating to the thesis.


Quantities obtainable from log files

The 'xxx-plt' programs (eg, 'net-plt') are the principal means by
which simulation runs are monitored.  These programs allow one to see
the values of various "quantities", evaluated for each iteration
stored in a log file within some range.  Some other programs (eg,
'xxx-hist') also use the same set of quantities.

A quantity is specified by an identifying character, perhaps with a
numeric modifier.  Some quantities are single numeric values
(scalars); others are arrays of values, in which case the desired
range of values is also specified.  Some quantities can be either
scalars or arrays, depending on whether a range specification is
included.

There is a hierarchy of quantities, as defined by modules at different
levels.  A few quantities are universally defined - principally 't',
the index of the current iteration.  Many more are defined for any
Markov chain Monte Carlo application - such as 'r', the rejection rate
for Metropolis or Hybrid Monte Carlo updates.  A large number of
quantities specific to neural networks are also defined - for example,
'b', the average squared error on the training set, and 'n', the
current value of the noise standard deviation (for a regression model).


SOME EXAMPLES OF BAYESIAN NETWORK TRAINING

This section shows how Bayesian training for neural network models can
be done for three simple synthetic problems.

The output shown below was obtained by running the software on our
machine, with ">" at the start of a line indicating a command line
that was input.  It is possible (even likely) that your results will
differ, even if you have installed the software correctly, since small
differences in floating point arithmetic can be magnified into large
differences in the course of the simulation.  However, unless one of
the simulations became stuck in an isolated local mode, the final
predictions you obtain from 'net-pred' should be close to those
reported below.

All the data sets mentioned here are present in the 'examples'
sub-directory, along with the C source of the programs that generated
them.  It is assumed below that you have changed to this directory.
The command sequences for running the simulations that are mentioned
below are also stored in this directory, in shell files with the names
'rcmds', 'bcmds', and 'ccmds'.

Note that the particular network architectures, priors, and Markov
chain sampling options used below are only examples of reasonable
choices.  There are many other possibilities that are also reasonable.
To gain a full understanding of the various possibilities, and their
advantages and disadvantages, you will need to read both my thesis and
the detailed documentation in the ".doc" files.


A simple regression problem

As a first example, we will look at a simple regression problem, 
in which there is one real-valued input for each case, and one
real-valued target, whose value is to be predicted.

I generated synthetic data of this type in which the input variable,
x, for each case had a standard Gaussian distribution and the
corresponding target value came from a Gaussian distribution with
standard deviation 0.1 and mean given by

         0.3 + 0.4*x + 0.5*sin(2.7*x) + 1.1/(1+x^2)

I generated 200 cases in total, stored in the file 'rdata'.  Each case
consists of a line containing first the input value and then the
target value.  The first 100 of these cases are meant for training,
and the second 100 for testing.

We will model this data using a multilayer perceptron with one input
unit, one hidden layer of eight tanh units, and a single output unit
whose activation function is the identity.  The value of the output
unit will be taken as the mean of a Gaussian distribution for the
target, with the standard deviation of this Gaussian (the noise level)
being a hyperparameter to be estimated along with the parameters of
the network.  We will also use hyperparameters to express the prior
distributions for the network parameters.  Specifically, we will use
one hyperparameter for the input-to-hidden weights, one for the hidden
unit biases, and one for the hidden-to-output weights.  The output
unit bias will be given a simple Gaussian prior, with no adjustable
hyperparameter.  (The role of hyperparameters is primarily to
introduce dependencies between parameters, so they are usually not
used when they would control just a single parameter.)

The first step in applying this model to the data is to create a log
file containing the specifications for the network architecture, data
model, and priors.  This can be done using the following command:

    > net-spec rlog 1 8 1 / - 0.05:0.5 0.05:0.5 - x0.05:0.5 - 100 \
                          / real 0.05:0.5

Here, 'rlog' is the name of the new log file, and the arguments "1",
"8", and "1", specify the numbers of input, hidden, and output units.
Following the "/", the priors for the various groups of network
parameters are given, with a "-" indicating that a parameter group
should be omitted (equivalent to the parameters being zero).  The
order of the groups is a bit hard to remember, but you can get a
summary easily by just invoking 'net-spec' with no arguments.  The
groups in the above command that are not omitted are the input-hidden
weights, the hidden biases, the hidden-output weights, and the output
bias.  Following another "/" (shown here after a "\" that indicates a
continuation of the command), the model to be used for the targets is
specified.  In this case, the targets are real-valued, and are modeled
as the network output plus Gaussian noise, with the noise standard
deviation being a hyperparameter having the prior given by the last
argument of the command.

The general form of the prior specifications mentioned above is a
"width" value followed by up to three "alpha" values, with perhaps an
option character tacked on to the front.  For the full details, see
Appendix A of my thesis and 'net-prior.doc'.  Here, I will just
comment on the particular priors used above.

The prior specification used for the output bias is simply "100",
which means that the bias has a Gaussian prior with mean zero and
standard deviation 100.  The prior specifications of the form
"0.05:0.5" indicate that the parameters in these groups are associated
with a hyperparameter, which gives the standard deviation of a
Gaussian prior for these parameters.  The hyperparameter itself has a
rather vague prior that spans several orders of magnitude around one.
The inverse gamma priors used are somewhat difficult to visualize,
because their tails are asymmetrical, but some standard choices are
often appropriate.  Here, the "0.5" after the colon controls how vague
the prior is (closer to zero is more vague).  The "0.05" specifies the
location of this vague distribution, but due to the asymmetry of the
tails, it is closer to being the lower limit of the prior than the
centre (for vague priors such as this).

The "x" in front of the prior for the hidden-to-output weights
indicates that the prior should be automatically rescaled based on the
number of hidden units, so as to produce an effect that is independent
of the number of hidden units (in the limit of large numbers).

A prior specification of the same form appears in the data model part,
after the "real" argument.  It controls the prior for the standard
deviation of the Gaussian noise distribution.

You can view the architecture and prior specifications stored in the
log file by invoking 'net-spec' with just a log file argument.  In
this example, this should give the following result:

    > net-spec rlog
    
    Network Architecture:
    
      Size of input layer:    1
      Sizes of hidden layers: 8
      Size of output layer:   1
    
      Data model: real
    
    
    Prior Specifications:
    
             Hidden Layer 0
    
      Input-Hidden Weights:    0.050:0.50
      Hidden Biases:           0.050:0.50
    
             Output Layer
    
      Hidden0-Output Weights: x0.050:0.50
      Input-Output Weights:    0.050:0.50
      Output Biases:           100.000
    
      Noise prior:   0.050:0.50

Next, we need to specify the data sets to be used for training and
(optionally) for testing.  We do this using the 'data-spec' command:

    > data-spec rlog 1 1 / rdata@1:100 . rdata@101:200 .
    Number of training cases: 100
    Number of test cases: 100

Here, "rlog" is the name of the log file we created with 'net-spec',
to which the data specifications will be appended.  The "1" and "1"
arguments give the numbers of inputs and targets.  These must be
consistent with the network architecture (if not, an error will be
reported later when you try to start the training).  

After the "/", specifications for where to get the training and test
data are given.  Each such specification consists of two parts: the
source for the inputs, and the source for the targets.  The
specification "rdata@1:100" means that the training inputs come from
the file 'rdata', in lines 1 to 100, while the specification of
"rdata@101:200" for the test inputs indicates that they also come from
the file 'rdata', but in lines 101 to 200.  In the above command, the
sources for the targets are given as just ".", which means the target
items are on the same lines as the inputs, following the last input
item.  We could have said that the targets come from a completely
different file, however.  It is also possible to specify exactly where
on a line the inputs and targets are located (and hence to ignore some
items in the file).  For documentation on these and other features,
see 'numin.doc'.

Though it is not done above, the 'data-spec' command also allows you
to specify transformations to be applied to the inputs or targets
before they are used.  This is useful, for example, if you wish to use
inputs that have been "normalized" to have mean zero and variance one,
based on the training data.  See 'data-spec.doc' for the details.

In the training runs reported in the thesis, I used a short "initial
phase" to get things started, followed by a long "sampling phase" to
bring the simulation to equilibrium and then produce a sample of
networks from the posterior for use in prediction.  I still use the
same general procedure, but with some changes to how the initial phase
is done.

It seems desirable to start the simulation in a state where the
hyperparameters take on moderate values, and leave them fixed for a
few iterations so that the network parameters will also take on
moderate values.  This can be accomplished using the following
commands:

    > net-gen rlog fix 0.5
    > mc-spec rlog repeat 10 sample-noise heatbath hybrid 100:10 0.2
    > net-mc rlog 1

The 'net-gen' command stores a network in the log file with index
zero, in which the hyperparameters have values of 0.5, and the network
parameters are zero.  This is the initial state of the simulation run.
The following 'mc-spec' command specifies the Markov chain operations
to be performed in the initial phase.  Here, each iteration consists
of ten repetitions of the following steps:\ \ Gibbs sampling for the
noise level, a heatbath replacement of the momentum variables, and a
hybrid Monte Carlo update with a trajectory 100 leapfrog steps long,
using a window of 10, and a stepsize adjustment factor of 0.2.  Note
that the hyperparameters are not updated, and hence will remain fixed
at values of 0.5.  Finally, a single such iteration is done by calling
'net-mc' with an iteration limit of 1.

The stepsize adjustment factor of 0.2 used above is typical of what is
needed, but will not be appropriate in all circumstances.  After the
'net-mc' command has finished, the number of the 10 hybrid Monte Carlo
updates that were rejected can be determined using the command
'net-plt t r rlog', which will write the iteration number (of 1) and
the rejection rate on standard output.  If the rejection rate is high
(say, over 0.3), a new run should be done using a smaller stepsize
adjustment factor.  In the initial phase, one would generally start by
guessing a value for the stepsize adjustment factor that is on the low
side, since there is no point in optimizing this choice.

At this point, we hope to have a network stored in the log file (with
index 1) that has values for both the parameters and hyperparameters
that are of moderate magnitude, and which have adapted at least
somewhat to the training data.  We can now start serious sampling with
the following commands:

    > mc-spec rlog sample-sigmas heatbath hybrid 1000:10 0.4
    > net-mc rlog 100

The 'mc-spec' command appends a new set of Markov chain operations to
the log file, which will override the previous set.  These operations
are Gibbs sampling for both the hyperparameters and the noise level
(the "sigmas"), a heatbath update for the momentum variables, and a
hybrid Monte Carlo update with a trajectory 1000 leapfrog steps long,
a window of 10, and a stepsize adjustment factor of 0.4.  A long
trajectory length is typically desirable for the sampling phase.  As
in the initial phase, the stepsize adjustment factor of 0.4 used is
typical, but not universally applicable.  It may pay at this stage to
experiment in order to find out how large this factor can be while
keeping the rejection rate low.  The use of a "window" of around 10
states costs little and is often beneficial (R. M. Neal, Journal of
Computational Physics, vol. 111, pp. 194-203).

The 100 iterations of the sampling phase started with the command
'net-mc rlog 100' will take a while to complete (about six minutes on
our SGI machine).  While you wait, you can look at the last network
saved in the log file (or any earlier one) using 'net-display'.  For
example, after about a minute, you might see the following:

    > net-display rlog
  
    Network in file "rlog" with index 15
    
    Input to Hidden Layer 0 Weights [1]
    
     3.36 3.36: -1.24  +1.73  -4.04  -0.24  +2.49  -2.95  -1.56  -3.21
    
    Hidden Layer 0 Biases [2]
    
          1.90: -1.58  -3.59  +4.42  +0.85  -4.59  -3.69  +2.03  -0.40
    
    Hidden Layer 0 to Output Weights [3]
    
     0.76 0.76: -1.23
    
          0.76: +1.04
    
          0.76: +0.34
    
          0.76: -0.38
    
          0.76: -0.24
    
          0.76: +0.80
    
          0.76: +0.52
    
          0.76: -0.56
    
    Output Biases [4]
    
        100.00: +1.10
    
    Noise levels
    
       0.08 -  0.08

This display of network parameters and hyperparameters is divided into
sections for different parameter groups.  Within each section, the
numbers before the colons are hyperparameters, those after are
parameters (weight and biases).  There are more hyperparameters shown
than were mentioned earlier, but for this network architecture, the
extra hyperparameters are either fixed in value (the 100 for output
biases), or tied to the value of a higher-level hyperparameter, so
they are effectively not present.

The parameter groups in the 'net-display' output are identified by
numbers in square brackets.  These can be used with the 'h', 'w', 
and 'W' quantities of 'net-plt'.  For example, to see how the
hyperparameter controlling the hidden-to-output weights has changed
during the simulation (so far), one can use the command

    > net-plt t h3 rlog | plot

where 'plot' is some suitable plot program.  (One can also just invoke
net-plt and look at the numbers printed on standard output.)  Here
'h3' refers to the top-level hyperparameter for group 3, which is seen
in the output of 'net-display' above to be the hidden-to-output group.

By looking at plots of the hyperparameters and quantities such as the
squared error on the training set ('b'), one can get an idea of when
the simulation has reached equilibrium.  Networks from that point on
can then be used to make predictions for test case using the
'net-pred' program.  Often, it will not be completely clear that
equilibrium has been reached until the simulation has been allowed to
proceed for quite a long time, but predictions based on shorter runs
may nevertheless be quite good.

For this problem, let's assume that we have decided to discard the
first 20 iterations as perhaps not coming from the equilibrium
distribution.  The following command will use the networks from the
remaining 80 iterations to produce predictions for all test cases, and
report the average squared error:

    > net-pred itn rlog 21: 

    Number of networks used: 80

    Case  Inputs  Targets  Means   Error^2

       1    0.92    1.49     1.59  0.0093
       2    0.71    1.83     1.79  0.0012
       3    0.20    1.72     1.67  0.0021
       
            ( midde lines omitted )
    
      98   -0.69    0.35     0.35  0.0000
      99   -1.33    0.19     0.36  0.0277
     100   -0.09    1.31     1.24  0.0052

    Average squared error guessing mean:   0.01035+-0.00147

The options "itn" specified ask for a listing of the inputs ("i") and
targets ("t") for each case, along with the mean ("n") output for that
case of the 80 networks used for prediction.  The squared error when
using this mean to predict the target is shown for each case, and the
average squared error for the test cases is shown at the bottom, along
with its standard error with respect to the random selection of test
cases.  Considering that the average squared error with optimal
prediction is 0.01 (due to the noise of standard deviation 0.1 added
when generating the data), the network model has done quite well, as
one would hope it would on an easy problem such as this.

It is also possible to get predictions for cases that are not in the
test set that was specified with 'data-spec'.  For example:

    > net-pred nb rlog 11: / "%echo 2.3"
        1.37

Here, the options "nb" ask for only the predictive mean, with "bare"
output (no headings).  The argument at the end says that the inputs
for test cases (here, just one case) should be taken from the output
of the Unix command "echo 2.3", which just outputs the number 2.3.


A problem with a binary response

As a second example, I generated a data set with two real-valued
inputs, x1 and x2, and a binary target.  The inputs were drawn
independently from standard Gaussian distributions.  The target was
then set to "1" with the following probability:

    exp ( - ((x1-0.4)^2 + (x2+0.3)^2) ^ 2 )

ie, the negative exponential of the fourth power of the distance of
(x1,x2) from (0.5,-0.3).  I generated 500 cases, and used the first
300 for training and the remaining 200 for testing.

For this problem, we can again try using a network with one layer of
hidden units (fifteen for this problem), and a single output unit.
For a binary target, a Gaussian data model would be inappropriate;
instead, we can use a logistic regression model, in which the
probability of the target being "1" is obtained by passing the value
of the output unit through the logistic function, f(x) = 1/(1+exp(-x)).

The network and data specification commands needed are quite similar
to those used in the regression problem above:

    > net-spec blog 2 15 1 / - 0.05:0.5 0.05:0.5 - x0.05:0.5 - 100 \
                           / binary
    > data-spec blog 2 1 2 / bdata@1:300 . bdata@301:500 .
    Number of training cases: 300
    Number of test cases: 200

The 'net-spec' command differs only in the number of input units (2),
the number of hidden units (15), and the data model, which here is
"binary" (with no need for a noise level prior).

The 'data-spec' command also says that there are two inputs, and one
target.  It also has a third argument of "2" just before the "/",
which indicates that the target must be an integer with two possible
values (which are "0" and "1").

The initial phase commands that were used for the simple regression
problem turn out to be adequate for this problem as well:

    > net-gen blog fix 0.5
    > mc-spec blog repeat 10 sample-noise heatbath hybrid 100:10 0.2
    > net-mc blog 1

For the sampling phase, we could also try using commands similar to
those presented above, but we might as well try something different:

    > mc-spec blog repeat 10 sample-sigmas heatbath 0.95 \
                             hybrid 100:10 0.3 negate
    > net-mc blog 200

The 'mc-spec' command specifies a variation on hybrid Monte Carlo due
in which relatively short trajectories are used, but random walks are
suppressed by only partially replacing the momentum in the 'heatbath'
step (A. M. Horowitz, Physics Letters B, vol. 268, pp. 247-252).  Note
that for this to work well, the rejection rate must be quite small.
This alternative method has the advantage that it allows for more
frequent hyperparameter updates (with 'sample-sigmas').

On our SGI machine, the 200 iterations requested above take about 70
minutes.  During this time, one can monitor the simulation, for
instance with the command:

    > net-plt t h1h2h3 blog | plot

where again, 'plot' is some appropriate plotting program, which in
this case must be capable of plotting three superimposed graphs, for
the three hyperparameters h1, h2, and h3.  The values for these
hyperparameters exhibit quite a high degree of autocorrelation, which
is why it is advisable to allow the simulation to go for 200
iterations, in order to be more confident that the simulation has
explored all high-probability regions of the posterior distribution.

Once the run has finished, we can make predictions using 'net-pred',
based, say, on the networks from the final 3/4 of the run (which is a
generally reasonable portion).  For a problem such as this, where the
response is binary, we are most likely interested in guessing whether
the target is "0" or "1", with performance measured by how often we
are right.  We can get such predictions as follows:

    > net-pred itm blog 51: 

    Number of networks used: 150

    Case  Inputs         Targets  Guesses  Wrong?

       1   -1.56   0.90    0.00       0       0
       2    0.09  -0.13    1.00       1       0
       3   -0.79   0.85    0.00       0       0
       
               ( midde lines omitted )
    
     198    1.49   0.70    0.00       0       0
     199   -2.23  -0.28    0.00       0       0
     200   -0.91  -0.03    0.00       0       0

    Fraction of guesses that were wrong:  0.0900+-0.0203

The "itm" options ask for output listing the inputs, the targets, and
the guesses based on the mode of the predictive distribution.  A
summary is also printed that reports the fraction of guesses that were
wrong, along with the standard error for this estimate of the error rate.


A three-way classification problem

As a final example, I created a synthetic data set for a three-way
classification problem.  Data items were generated by first drawing
quantities x1, x2, x3, and x4 independently from distributions uniform
over (0,1).  The class of the item, represented by "0", "1", or "2",
was then selected as follows:  if the two-dimensional Euclidean
distance of (x1,x2) from the point (0.4,0.5) was less than 0.35, the
class was set to "0"; otherwise, if 0.8*x1+1.8*x2 was less than 0.6,
the class was set to "1"; and if neither of these conditions held, the
class was set to "2".  Note that x3 and x4 have no effect on the
class.  The class selected by this procedure was the target value for
the network; the inputs available to the network were not x1, x2, x3,
and x4 themselves, however, but rather these four values plus Gaussian
noise of standard deviation 0.1.  I generated 1000 cases in this way,
of which 400 were used for training and 600 for testing.

This example will illustrate the "softmax" model for target values in
a small set, and the Automatic Relevance Determination (ARD) prior for
input-to-hidden weights.  The network specification used is as follows:

    > net-spec clog 4 8 3 / - 0.05:0.5:0.5 0.05:0.5 - x0.05:0.5 - 0.05:0.5 \
                          / class

This specifies a network with 4 input units, 8 hidden units, and 3
output units.  The output units have identity activation functions,
but their values are used in a "softmax" data model, in which the
probability for target i is exp(o_i) / SUM_j exp(o_j), where the o_j
are the values of the output units.

The prior specified for input-to-hidden weights, "0.05:0.5:0.5", has
two "alpha" values, indicating that there is both a high-level
hyperparameter controlling the overall magnitude of input-to-hidden
weights, and lower-level hyperparameters for each input unit, which
control the magnitudes of weights out of each input.  If some of the
inputs are irrelevant (as are the third and fourth inputs in this
problem), we hope that the corresponding hyperparameters will take on
small values, forcing the weights out of these inputs to be close to
zero, and thereby avoiding any damage to predictive performance that
could result from the inclusion of these irrelevant inputs.

We need to use the following data specification for this problem:

    > data-spec clog 4 1 3 / cdata@1:400 . cdata@401:1000 .
    Number of training cases: 400
    Number of test cases: 600

The arguments "4" and "1" are the numbers of input and target values.
The "3" before the "/" says that each target must be one of the
integers "0", "1", or "2".  This "3" must match the number of softmax
output units specified in the network architecture, or an error will
result when training is started.

The initial and sampling phases of the simulation can be done using
the same approach as was used above for the binary data set:

    > net-gen clog fix 0.5
    > mc-spec clog repeat 10 sample-noise heatbath hybrid 100:10 0.2
    > net-mc clog 1
    > mc-spec clog repeat 10 sample-sigmas heatbath 0.95 \
                             hybrid 100:10 0.3 negate
    > net-mc clog 200

This takes about 75 minutes on our SGI machine.

We can see the effect of the ARD model by looking at the values of
the low-level input-hidden hyperparameters over the course of the
simulation, with the command:

    > net-plt t h1@ clog | plot

The third and fourth hyperparameters are much smaller than the first
two, indicating the ARD has done its job.

Predictions for test cases based on the predictive mode can now be
done in the same way as for the binary response problem.  If we are
interested only in the estimated classification performance, we can
use the following command:

    > net-pred ma clog 51:

    Number of networks used: 150

    Number of test cases: 600

    Fraction of guesses that were wrong:  0.1400+-0.0142

Here, the "a" option is used to suppress everything except the summary.


HINTS AND WARNINGS

1) The message "WARNING: Over 10000 tries in one call of cond_sigma"
   will be printed on standard error if an attempt at Gibbs sampling
   for a high-level hyperparameter using the rejection sampling 
   scheme described in section A.2.2 of the thesis has resulted in 
   over 10000 rejections in a single Gibbs sampling update.  The 
   program will continue to try indefinitely, without printing any 
   further warnings, but it is possible that progress from this point
   on will be horrendously slow.  (On the other hand, this message
   can also arise from occasional sampling difficulties that do not 
   produce a significant overall slowdown.)

2) When there is a single output unit, a specification of the form
   "w:a:b" for the hidden-to-output weights is mathematically
   equivalent to one of the form "w:a::b".  The two specifications
   differ computationally, however.  In the "w:a:b" form, lower-level
   hyperparameters that each control a single weight are explicitly
   represented; with "w:a::b", equivalent hyperparameters exist
   mathematically, but are not represented explicitly.  The "w:a:b"
   form is probably to be preferred, since explicit hyperparameters 
   are of assistance to the heuristic procedure that chooses
   stepsizes for the dynamical updates.

3) If the system crashes in the middle of a run of 'net-mc', one can
   usually continue from the last iteration written to the log file 
   by just invoking 'net-mc' again with the same arguments (just as 
   one can continue for more iterations after 'net-mc' terminates 
   normally).  Problems could arise if the system crashed in the middle 
   of writing the records pertaining to an iteration, in which case some 
   fixup using 'log-copy' may be required.  Such problems could come
   either from a partial record at the end of the log file, or from
   a less-than-complete set of full records.  It is best to assess the 
   situation using 'log-records' before proceeding.


FURTHER DOCUMENTATION

The overview and examples above are intended just to get you started.
To use the software to do real work, you will probably need to refer
to the detailed documentation on the commands (and common aspects of
commands) contained in the files ending with ".doc" in the various
sub-directories.  For quick reference, all commands print a brief
summary of the command syntax when they are invoked with no arguments.

In the syntax descriptions used, the characters "[" and "]" enclose
parts of the command that are optional, "{" and "}" enclose optional
parts that can be repeated, and "|" separates alternatives.  Except
for the command name, the words in the syntax descriptions are
descriptive of what is to be entered, except that words in quotes are
to be entered literally (without the quotes).

The ".doc" files present in the various directories are listed below,
with the more important files marked by "*".  Programs listed as
"xxx-something" are generic, with "xxx" being replaced by the name of
an application (eg, "net" or "bvg").  In some cases, further
documentation is available under the specific name.

The file doc-index.html is a hypertext index to this documenation.  It
can be accessed using a Web browser (eg, xmosaic), by a command such as

    xmosaic doc-index.html

The doc-index.html file must reside in the main directory for this
software, with the various .doc files in the appropriate sub-directories.  
One needn't start xmosaic from this directory, however, provided one
uses a full path name to identify doc-index.html.


Generic utilitiy programs [util]:

  * log             Facilities for handling log files
    log-types       Types of log file records used by various programs

    log-copy        Copy part of a log file to a new log file
    log-last        Display the index of the last record in a log file
    log-records     List all records in a log file  

  * data-spec       Specify data sets for training and testing
  * numin           Facilities for input of numeric data
    numin-test      Test numeric input module

    grid            Output a grid of points  
    extract         Extract items at random from a data file 

  * rand-seed       Specify a random number seed  
    rand-test       Test random number generators

  * quantities      Numeric quantities obtainable from log files

  * xxx-plt         Write quantities from a log file, suitable for plotting
    xxx-hist        Build a histogram for a quantity obtained from a log file

  * series          Analyse stationary time series data


Markov chain Monte Carlo facilities [mc]:

  * mc              Programs and modules supporting Markov chain Monte Carlo 
  * mc-spec         Specify how to do the Markov chain simulation
  * xxx-mc          Run Markov chain simulation

  * mc-quantities   Quantities from log files relating to Monte Carlo 

    mc-temp-sched   Specify temperature schedule for tempering methods
    mc-temp-filter  Copy only iterations at a given temperature

    xxx-grad-test   Test the correctness of the energy gradient computations
    xxx-stepsizes   Display and evaluate stepsizes used for dynamics


Bayesian neural networks [net]:

  * net             Bayesian inference for neural networks using MCMC
  * net-spec        Create a new network, or display existing specifications 
  * net-prior       Form of prior specifications for network models

  * net-mc          Do Markov chain simulation to sample networks
  * net-gen         Generate networks from the prior, or with fixed values  

  * net-display     Print network parameters and/or hyperparameters

  * net-quantities  Quantities from log files relating to networks
  * net-plt         Get quantities from a net log file, suitable for plotting
    net-hist        Build histogram for quantity obtained from a net log file

  * net-pred        Make predictions for test cases

    net-eval        Evaluate network functions over a grid  
    net-dvar        Find the variance of a difference in function values

    net-rej         Generate networks from the posterior by rejection sampling


Markov chain sampling for a bivariate Gaussian [bvg]:

    bvg             Demo of Markov chain sampling from a bivariate Gaussian
    bvg-spec        Specify a bivariate Gaussian distribution to sample from
    bvg-mc          Do Markov chain simulation for a bivariate Gaussian
    bvg-plt         Get quantities from a bvg log file, suitable to plot


ACKNOWLEDGEMENTS

The features allowing cpu time to be stored, displayed, and used to
control the number of iterations are based on work by Carl Edward
Rasmussen.  Thanks also to David MacKay and Chris Williams for testing
the programs on their machines, and for comments on the documentation.
